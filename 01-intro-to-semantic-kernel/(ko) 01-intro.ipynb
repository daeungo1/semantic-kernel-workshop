{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Semantic Kernel 소개\n",
    "\n",
    "### Semantic Kernel(SK)의 개요와 중요성\n",
    "\n",
    "**Semantic Kernel**은 Microsoft에서 개발한 오픈소스 SDK로, 애플리케이션 코드와 AI 대규모 언어 모델(LLMs) 사이에서 미들웨어 역할을 한다. 이를 통해 AI 에이전트가 코드 함수를 호출하거나 복잡한 작업을 오케스트레이션할 수 있어, 애플리케이션에 AI 기능을 부드럽게 통합할 수 있다.  \n",
    "SK는 *경량*이며 *모듈형*으로 설계되어 있고, 엔터프라이즈 요구사항에 맞춘 **텔레메트리**, **Responsible AI 필터링**, **보안 기능** 등을 제공한다. Microsoft를 포함한 많은 기업이 SK를 활용하는 이유는 **유연성**, **확장성**, 그리고 **미래지향성(future-proof)** 때문이다. 새로운 AI 모델이 등장하더라도 코드의 큰 변경 없이 쉽게 교체할 수 있다.  \n",
    "요약하자면 SK는 **견고하고 확장 가능한 AI 애플리케이션**을 구축하는 데 최적의 프레임워크다.\n",
    "\n",
    "Semantic Kernel이 AI 애플리케이션 개발에서 중요한 이유는 다음과 같다:\n",
    "\n",
    "- **AI와 코드의 연결(Bridging AI and Code)**  \n",
    "  SK는 자연어 기반 **prompts**와 기존 **코드 및 API**를 결합해 AI가 실제 행동(action)을 수행할 수 있도록 한다.  \n",
    "  AI 모델이 함수 호출을 요청하면 SK가 해당 함수를 실행하고 결과를 모델에 돌려준다.  \n",
    "  이는 AI의 “의도(intention)”와 실제 코드 실행 사이의 간극을 줄여준다.\n",
    "\n",
    "- **Plugins(Skills)**  \n",
    "  간단한 수학 함수부터 복잡한 비즈니스 로직, 외부 API 호출까지 모든 기능을 SK **plugin** 형태로 노출할 수 있다.  \n",
    "  함수 정의를 통해 AI가 기능을 이해하고 필요 시 자동으로 호출한다.  \n",
    "  이러한 구조는 솔루션을 **모듈화**하고 **확장성**을 크게 향상시킨다.\n",
    "\n",
    "- **Enterprise-ready 기능**  \n",
    "  SK는 **보안(Security)**, **관측성(Observability)**, **컴플라이언스(Compliance)** 기능을 제공하며 Azure 서비스와 쉽게 통합된다.  \n",
    "  Content Filtering, Logging hooks 등을 통해 민감 정보 노출을 방지하는 정책도 적용할 수 있다.\n",
    "\n",
    "- **멀티모달 & 미래지향적(Multi-modal & Future-Proof)**  \n",
    "  SK는 OpenAI, Azure OpenAI, HuggingFace 등 다양한 모델 공급자를 기본 지원한다.  \n",
    "  텍스트 기반 모델뿐 아니라 이미지·음성 등 멀티모달 확장도 가능하며,  \n",
    "  새로운 AI 모델이 출시되더라도 구조 변경 없이 손쉽게 교체할 수 있다.\n",
    "\n",
    "- **빠른 개발(Rapid Development)**  \n",
    "  Prompt orchestration, Function Calling, Memory Management 등 복잡한 작업을 SK가 처리한다.  \n",
    "  개발자는 “무엇을 할지(What)”만 정의하면 되고, “어떻게 할지(How)”는 SK가 처리한다.  \n",
    "  Microsoft는 SK가 **가장 빠르게 AI 기능을 구현할 수 있는 SDK**라고 강조한다.\n",
    "\n",
    "---\n",
    "\n",
    "### SK의 서비스 및 핵심 컴포넌트\n",
    "\n",
    "Semantic Kernel의 아키텍처는 다음과 같은 핵심 요소들로 구성된다:\n",
    "\n",
    "- **Kernel**  \n",
    "  SK의 중심 객체로, AI 서비스 설정 관리, Plugin(Skill) 로딩, 함수 호출 조정, 메모리 유지 등을 담당한다.  \n",
    "  보통 애플리케이션에서 Kernel 인스턴스를 하나 생성해 모든 AI 요청을 처리한다.\n",
    "\n",
    "- **AI Services**  \n",
    "  SK는 다양한 종류의 AI 모델과 연결하여 각각 다른 기능을 수행한다:\n",
    "  - *Chat Models*: 예) Azure OpenAI GPT-4o-mini, GPT-4o  \n",
    "    자연어 생성/이해 처리  \n",
    "  - *Embedding Models*: 텍스트를 벡터 embedding으로 변환해 검색/메모리 기능에 사용  \n",
    "  - *Other Modalities*: 필요할 경우 이미지·음성 등 확장 가능\n",
    "\n",
    "Semantic Kernel은 프로젝트 루트의 `.env` 파일을 자동으로 읽어 Azure OpenAI 설정을 불러올 수 있다.  \n",
    "필요한 환경 변수는 다음과 같다:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_ENDPOINT - 기본적으로 사용할 Azure OpenAI 엔드포인트\n",
    "AZURE_OPENAI_API_KEY - 사용할 API Key\n",
    "AZURE_OPENAI_API_VERSION - 기본적으로 사용할 Inference API 버전\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME - 기본 Chat Model 배포 이름\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME - 기본 Embedding Model 배포 이름\n",
    "```\n",
    "\n",
    "`.env.example`을 복사해 생성한 `.env` 파일이 정확하게 입력되어 있는지 반드시 확인해야 한다.\n",
    "\n",
    "이제 이러한 endpoint와 key 정보를 사용해 Kernel에 필요한 AI 서비스를 구성할 수 있다.  \n",
    "예를 들어 Azure OpenAI Chat Completion 서비스를 Kernel에 추가하는 예시는 다음과 같다:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantic-kernel==1.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계로 진행하기 전에, Semantic Kernel(SK)이 올바르게 로드될 수 있도록 Jupyter 커널을 한 번 재시작해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion = AzureChatCompletion()\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커널 생성과 Chat Completion 서비스를 성공적으로 추가했습니다.\n",
    "\n",
    "마찬가지로, 시맨틱 메모리 검색을 수행할 경우 `kernel.add_service(text_embedding)`을 통해 Embedding 생성 서비스를 추가할 수도 있습니다. 이 부분은 이후 단계에서 자세히 다룰 예정이니 걱정하지 않아도 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### SK의 Functions와 Plugins\n",
    "\n",
    "Semantic Kernel에서 **Function(함수)**는 AI가 수행할 수 있는 동작을 의미합니다. 함수는 두 가지 유형으로 구성됩니다:\n",
    "\n",
    "- **Semantic Functions**: Prompt와 LLM을 기반으로 동작하는 함수  \n",
    "  예: `\"Translate this to French: {{$input}}\"` 프롬프트를 사용하는 `translateToFrench` 함수\n",
    "\n",
    "- **Native Functions**: 실제 코드로 구현된 함수  \n",
    "  예: `sendEmail(to, subject, body)` 같이 이메일 API를 호출하는 Python 함수\n",
    "\n",
    "이러한 함수들은 보통 **Plugin(Skill)**로 묶여 관리됩니다. Python에서는 하나의 클래스가 하나의 Plugin을 나타내는 방식이 일반적입니다. Plugin 단위로 기능을 묶으면, AI에게 어떤 기능을 노출할지 더 명확하게 제어할 수 있습니다.\n",
    "\n",
    "**Plugin/Function 사용하기**  \n",
    "Kernel에 함수가 등록되면(`kernel.add_function`, `kernel.add_plugin`) 해당 함수는 호출 가능한 상태가 됩니다.  \n",
    "- 코드에서 직접 호출할 수 있고: `kernel.invoke(function, input)`  \n",
    "- 또는 AI 모델이 필요에 따라 자동으로 호출할 수도 있습니다.\n",
    "\n",
    "SK의 Plugin 시스템은 매우 유연합니다:\n",
    "- **OpenAPI 스펙**이나 API 엔드포인트를 그대로 Plugin으로 로드할 수 있고  \n",
    "- Plugin을 여러 프로젝트에서 공유해 조직 차원의 AI 기능 라이브러리를 구축할 수도 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Kernel에 Plugin을 추가하는 방식은 여러 가지가 있습니다:\n",
    "\n",
    "- **Inline Definition**: 코드 안에서 직접 prompt 또는 함수 정의 후 등록  \n",
    "- **From Files or Classes**: 디렉터리에서 불러오거나 데코레이터가 적용된 클래스 로드\n",
    "\n",
    "아래는 간단한 **semantic function**을 inline으로 추가하는 예시입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a semantic function (prompt) to generate a TL;DR summary\n",
    "prompt_template = \"{{$input}}\\n\\nTL;DR in one sentence:\"\n",
    "\n",
    "summarize_fn = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"tldr\",\n",
    "    plugin_name=\"Summarizer\",\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "long_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(summarize_fn, input=long_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a translation function\n",
    "\n",
    "텍스트를 원하는(target) 언어로 번역하는 semantic function을 작성하세요.\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "  # Define a semantic function (prompt) to generate a translation\n",
    "prompt_template = \"{{$input}}\\n\\nTranslate this into {{$target_lang}}:\"\n",
    "\n",
    "translate_fn = kernel.add_function(\n",
    "    prompt=prompt_template, \n",
    "    function_name=\"translator\", \n",
    "    plugin_name=\"Translator\",\n",
    "    max_tokens=50)\n",
    "\n",
    "# Use the function\n",
    "text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(translate_fn, input=text, target_lang=\"French\")\n",
    "print(summary)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the previous implementation for reference, if you are stuck view the solution.\n",
    "\n",
    "# Your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 여러 개의 **native functions**를 포함한 Plugin을 만들고 싶다면 다음과 같이 할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependencies for the plugin\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"Please turn on the lamp\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Function Calling\n",
    "\n",
    "Semantic Kernel의 가장 강력한 기능 중 하나는 여러 함수를 순차적으로 호출하여 **다단계 작업을 자동으로 오케스트레이션**할 수 있다는 점입니다. 아래 예시에서는 **LightsPlugin**이 정의되어 있으며, 세 가지 비동기 native function을 포함합니다:\n",
    "\n",
    "- **get_lights()**: 조명 목록과 각 조명의 현재 상태를 가져옵니다.\n",
    "- **get_state(id)**: 특정 ID를 가진 조명의 상태를 반환합니다.\n",
    "- **change_state(id, new_state)**: 지정한 조명의 상태를 새로운 상태로 변경합니다.  \n",
    "  (예: 켜기, 밝기 조절, 색상 변경 등)\n",
    "\n",
    "**동작 방식**:\n",
    "- Kernel은 등록된 모든 함수를 AI 모델에게 노출합니다.\n",
    "- 사용자가 *\"모든 조명을 켜고 최종 상태를 알려줘\"* 같은 요청을 하면,  \n",
    "  AI 모델은 작업을 수행하기 위한 단계들을 스스로 계획합니다:\n",
    "  1. **get_lights()**를 호출해 모든 조명 목록을 가져옵니다.\n",
    "  2. 각각의 조명에 대해 **change_state()**를 호출해 `\"is_on\": True`와 같이 새로운 상태를 적용합니다.\n",
    "  3. 필요하다면 각 조명에 대해 **get_state()**를 호출해 상태가 제대로 업데이트되었는지 확인합니다.\n",
    "- Kernel은 마지막으로 각 조명의 최종 상태를 포함한 종합적인 결과를 반환합니다.\n",
    "\n",
    "**예시 시나리오**:\n",
    "- **사용자 요청**: *\"모든 조명 켜고 상태 알려줘.\"*\n",
    "- **Step 1**: 시스템이 `LightsPlugin.get_lights()`를 호출하여 현재 조명 목록을 조회합니다.\n",
    "- **Step 2**: 조명 목록을 순회하면서 `LightsPlugin.change_state(id, new_state)`를 호출하여 각각의 조명을 켭니다.\n",
    "- **Step 3**: 필요 시 `LightsPlugin.get_state(id)`를 호출하여 변경 사항을 확인합니다.\n",
    "- 마지막으로 모든 조명의 업데이트된 상태가 사용자에게 반환됩니다.\n",
    "\n",
    "이러한 자동 오케스트레이션 기능 덕분에 AI는 사람이 중간 단계마다 개입하지 않아도 스스로 작업 단계를 계획하고 함수 호출을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try other messages and observe the results\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Exercise: Creating a Weather Information Plugin\n",
    "\n",
    "이 연습에서는 Semantic Kernel을 위한 native plugin을 만들어 간단한 날씨 서비스(weather service)를 시뮬레이션해봅니다. 이를 통해 Plugin 안에서 native function을 만들고 사용하는 방법을 이해할 수 있습니다.\n",
    "\n",
    "### Task:\n",
    "1. `WeatherPlugin` 클래스를 만들고 아래 세 가지 함수를 구현하세요:\n",
    "   - `get_current_weather(location)`: 지정된 위치의 현재 날씨를 반환\n",
    "   - `get_forecast(location, days)`: 특정 일수(days)에 대한 날씨 예보를 반환\n",
    "   - `get_weather_alert(location)`: 해당 지역에 활성(weather alert)이 있는 경우 경보 내용 반환\n",
    "\n",
    "2. Plugin을 Kernel에 등록하고, 사용자 요청을 통해 테스트하세요.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "    from typing import Annotated, List, Dict\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    import random\n",
    "\n",
    "    class WeatherPlugin:\n",
    "        def __init__(self):\n",
    "            # Simulated weather data\n",
    "            self.weather_conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\", \"Windy\", \"Foggy\", \"Stormy\"]\n",
    "            self.temperature_ranges = {\n",
    "                \"New York\": (50, 85),\n",
    "                \"London\": (45, 75),\n",
    "                \"Tokyo\": (55, 90),\n",
    "                \"Sydney\": (60, 95),\n",
    "                \"Paris\": (48, 80),\n",
    "                \"Default\": (40, 100)\n",
    "            }\n",
    "            \n",
    "            # Simulated alerts\n",
    "            self.alerts = {\n",
    "                \"New York\": \"Heat advisory in effect\",\n",
    "                \"Tokyo\": \"Typhoon warning for coastal areas\",\n",
    "                \"Sydney\": None,\n",
    "                \"London\": None,\n",
    "                \"Paris\": \"Air quality warning\"\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_current_weather(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get weather for\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets the current weather for a specified location.\"\"\"\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            temperature = random.randint(temp_range[0], temp_range[1])\n",
    "            condition = random.choice(self.weather_conditions)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"temperature\": temperature,\n",
    "                \"condition\": condition,\n",
    "                \"humidity\": random.randint(30, 95),\n",
    "                \"wind_speed\": random.randint(0, 30)\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_forecast(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get forecast for\"],\n",
    "            days: Annotated[int, \"Number of days for the forecast\"] = 3\n",
    "        ) -> List[Dict]:\n",
    "            \"\"\"Gets a weather forecast for a specified number of days.\"\"\"\n",
    "            forecast = []\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            \n",
    "            for i in range(days):\n",
    "                forecast.append({\n",
    "                    \"day\": i + 1,\n",
    "                    \"temperature\": random.randint(temp_range[0], temp_range[1]),\n",
    "                    \"condition\": random.choice(self.weather_conditions),\n",
    "                    \"humidity\": random.randint(30, 95),\n",
    "                    \"wind_speed\": random.randint(0, 30)\n",
    "                })\n",
    "            \n",
    "            return forecast\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_weather_alert(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to check for weather alerts\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets any active weather alerts for a location.\"\"\"\n",
    "            alert = self.alerts.get(location)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"has_alert\": alert is not None,\n",
    "                \"alert_message\": alert if alert else \"No active alerts\"\n",
    "            }\n",
    "\n",
    "    # Usage example:\n",
    "    # Create the plugin\n",
    "    weather_plugin = WeatherPlugin()\n",
    "\n",
    "    # Register with kernel\n",
    "    kernel.add_plugin(\n",
    "        plugin=weather_plugin,\n",
    "        plugin_name=\"Weather\"\n",
    "    )\n",
    "\n",
    "    # Test with a user query\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What's the weather like in Tokyo and are there any alerts?\")\n",
    "\n",
    "    # Get response using function calling\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(\"Assistant > \" + str(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Learning Objectives:\n",
    "- 여러 개의 함수를 포함한 native plugin 생성하기\n",
    "- 함수 파라미터에 type annotation 사용하기\n",
    "- Plugin을 Kernel에 등록하는 방법 이해하기\n",
    "- 자연어 기반 사용자 쿼리를 통해 Plugin을 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Exercise\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters in Semantic Kernel\n",
    "\n",
    "Filters는 함수 실행에 대한 **제어(control)**와 **가시성(visibility)**을 제공하여, Responsible AI 원칙과 엔터프라이즈 환경의 보안 요구사항을 충족하도록 도와줍니다. Filters를 사용하면 다음과 같은 작업이 가능합니다:\n",
    "\n",
    "- **권한 검증(Validate Permissions):**  \n",
    "  예를 들어, 승인 플로우를 시작하기 전에 사용자의 권한을 필터에서 확인할 수 있습니다.\n",
    "\n",
    "- **함수 실행 가로채기(Intercept Function Execution):**\n",
    "\n",
    "  - **Function Invocation Filter:**  \n",
    "    함수가 호출될 때마다 실행됩니다.  \n",
    "    이 필터는 함수 정보에 접근할 수 있고, 예외 처리, 결과 수정(예: 캐싱 또는 Responsible AI 목적), 재시도 로직 등을 수행할 수 있습니다.\n",
    "\n",
    "  - **Prompt Render Filter:**  \n",
    "    프롬프트가 렌더링되기 전에 실행됩니다.  \n",
    "    이를 통해 프롬프트 내용을 확인하거나 수정할 수 있으며, 필요하면 프롬프트 제출 자체를 막기 위해 결과를 덮어쓸 수도 있습니다.\n",
    "\n",
    "  - **Auto Function Invocation Filter:**  \n",
    "    자동 함수 호출(Automatic Function Calling) 과정 내부에서 동작합니다.  \n",
    "    추가적인 컨텍스트(예: 채팅 히스토리, 반복 횟수 등)를 제공하며, 필요 시 프로세스를 조기에 종료할 수도 있습니다.\n",
    "\n",
    "각 필터는 실행 세부 정보를 담고 있는 **context 객체**를 전달받으며, 실행 체인을 계속 이어가기 위해 다음 delegate(또는 callback)을 반드시 호출해야 합니다.  \n",
    "필터는 `kernel.add_filter` 메서드로 등록하거나 `@kernel.filter` 데코레이터를 사용해 등록할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "우리가 지금까지 구현한 Plugin에서 개선하고 싶은 점 중 하나는 **디버깅 기능을 추가하는 것**입니다.  \n",
    "이 기능을 통해 외부 시스템과 연동하여 감사(auditing) 목적으로 로깅을 기록할 수 있게 됩니다.\n",
    "\n",
    "이제 이를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Awaitable, Callable\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "async def logger_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    logger.info(f\"FunctionInvoking - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "    await next(context)\n",
    "\n",
    "    logger.info(f\"FunctionInvoked - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "\n",
    "kernel.add_filter(\"function_invocation\", logger_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message(\"Please turn on all the lamps and give me their final state\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과는 다음과 비슷하게 나타날 것입니다.\n",
    "\n",
    "```shell\n",
    "2025-02-27 15:21:00,969 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:00,971 - INFO - OpenAI usage: CompletionUsage(completion_tokens=84, prompt_tokens=407, total_tokens=491, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "2025-02-27 15:21:00,971 - INFO - processing 2 tool calls in parallel.\n",
    "2025-02-27 15:21:00,972 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,972 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,973 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,973 - INFO - Function completed. Duration: 0.000904s\n",
    "2025-02-27 15:21:00,974 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,975 - INFO - Function completed. Duration: 0.000567s\n",
    "2025-02-27 15:21:06,333 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:06,335 - INFO - OpenAI usage: CompletionUsage(completion_tokens=120, prompt_tokens=572, total_tokens=692, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "Assistant > All the lights are now turned on. Here's the final state of each lamp:\n",
    "\n",
    "1. **Table Lamp**\n",
    "   - Status: On\n",
    "   - Brightness: 100%\n",
    "   - Color: Red (#FF0000)\n",
    "\n",
    "2. **Porch Light**\n",
    "   - Status: On\n",
    "   - Brightness: 50%\n",
    "   - Color: Green (#00FF00)\n",
    "\n",
    "3. **Chandelier**\n",
    "   - Status: On\n",
    "   - Brightness: 75%\n",
    "   - Color: Blue (#0000FF)\n",
    "\n",
    "If you need any further adjustments, just let me know!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Creating a Content Filter\n",
    "\n",
    "이 연습에서는 Semantic Kernel 파이프라인을 통과하는 콘텐츠를 **모니터링**하고 필요한 경우 **수정**하는 필터를 만들어봅니다. 이를 통해 AI 애플리케이션에서 안전성(safety)과 컴플라이언스(compliance) 기능을 구현하는 방법을 이해하게 됩니다.\n",
    "\n",
    "### Task:\n",
    "1. 신용카드 번호, 이메일 등 민감한 정보를 감지하고 마스킹(redact)하는 콘텐츠 필터를 생성하세요.\n",
    "2. 사용자 입력에 대해 **pre-processing 필터**, AI 출력에 대해 **post-processing 필터**로 적용하세요.\n",
    "3. 민감 정보를 포함한 여러 입력을 테스트하여 필터가 제대로 동작하는지 확인하세요.\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "import re\n",
    "from typing import Any, Coroutine\n",
    "from collections.abc import Callable as ABCCallable\n",
    "import logging\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "from semantic_kernel.functions import FunctionResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regular expressions for sensitive data patterns\n",
    "PATTERNS = {\n",
    "    'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',  # Credit card format: XXXX-XXXX-XXXX-XXXX\n",
    "    'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "    'phone': r'\\b(?:\\+\\d{1,3}[-\\s]?)?\\(?\\d{3}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\\b',  # Phone numbers\n",
    "    'ssn': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',  # Social Security Numbers (US)\n",
    "}\n",
    "\n",
    "class ContentFilter:\n",
    "    def __init__(self, patterns=PATTERNS):\n",
    "        self.patterns = patterns\n",
    "        \n",
    "    def redact_sensitive_info(self, text):\n",
    "        \"\"\"Redact sensitive information from text.\"\"\"\n",
    "        result = text\n",
    "        detected = []\n",
    "        \n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            matches = re.finditer(pattern, result)\n",
    "            for match in matches:\n",
    "                detected.append(f\"{pattern_name}: {match.group()}\")\n",
    "                result = result.replace(match.group(), f\"[REDACTED {pattern_name.upper()}]\")\n",
    "        \n",
    "        return result, detected\n",
    "\n",
    "# Create a pre-processing filter for user inputs\n",
    "async def input_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's an input parameter\n",
    "    if 'input' in context.arguments:\n",
    "        original_input = context.arguments['input']\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_input, detected = content_filter.redact_sensitive_info(original_input)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in input: {', '.join(detected)}\")\n",
    "            \n",
    "        # Replace the original input with the filtered version\n",
    "        context.arguments['input'] = filtered_input\n",
    "    \n",
    "    # Continue to the next filter or function\n",
    "    await next(context)\n",
    "\n",
    "# Create a post-processing filter for AI outputs\n",
    "async def output_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    # Continue to the next filter or function first\n",
    "    await next(context)\n",
    "    \n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's a result to filter\n",
    "    if context.result:\n",
    "        original_output = str(context.result)\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_output, detected = content_filter.redact_sensitive_info(original_output)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in output: {', '.join(detected)}\")\n",
    "         \n",
    "        # Create a new FunctionResult with the filtered output\n",
    "        context.result = FunctionResult(\n",
    "            function=context.function.metadata,\n",
    "            value=filtered_output,\n",
    "            metadata=context.result.metadata if hasattr(context.result, 'metadata') else {}\n",
    "        )\n",
    "\n",
    "async def test_content_filters():\n",
    "    \n",
    "    # Register the filters with the kernel\n",
    "    kernel.add_filter(\"function_invocation\", input_filter)\n",
    "    kernel.add_filter(\"function_invocation\", output_filter)\n",
    "    \n",
    "    # Create a simple semantic function\n",
    "    echo_prompt = \"{{$input}}\"\n",
    "    echo_fn = kernel.add_function(\n",
    "        prompt=echo_prompt,\n",
    "        function_name=\"echo\",\n",
    "        plugin_name=\"TestPlugin\"\n",
    "    )\n",
    "    \n",
    "    # Test with sensitive information\n",
    "    test_inputs = [\n",
    "        \"My credit card number is 4111-1111-1111-1111\",\n",
    "        \"Contact me at john.doe@example.com or call 555-123-4567\",\n",
    "        \"My SSN is 123-45-6789\",\n",
    "        \"Here's my info: john.doe@example.com, 4111-1111-1111-1111, 555-123-4567\",\n",
    "        \"What are the services that you can offer?\"\n",
    "    ]\n",
    "    \n",
    "    for input_text in test_inputs:\n",
    "        print(f\"\\nOriginal Input: {input_text}\")\n",
    "        result = await kernel.invoke(echo_fn, input=input_text)\n",
    "        print(f\"Filtered Output: {result}\")\n",
    "\n",
    "await test_content_filters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Objectives:\n",
    "- Semantic Kernel에서 필터를 생성하고 등록하는 방법 이해하기\n",
    "- Pre-processing / Post-processing 로직 구현하기\n",
    "- 정규 표현식(Regular Expressions)을 사용한 패턴 매칭 적용하기\n",
    "- 필터 실행 파이프라인(Filter Execution Pipeline)의 동작 방식 이해하기\n",
    "- 민감 정보 로깅 및 모니터링 기법 익히기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Filters in SK and Their Use Cases\n",
    "\n",
    "정리하자면, 어떤 AI 애플리케이션에서도 입력(input), 출력(output), 그리고 함수 실행(function execution)을 보안(Security), 프라이버시(Privacy), 그리고 정확성(Correctness)을 위해 제어하는 것이 매우 중요합니다.  \n",
    "Semantic Kernel의 **Filters**는 실행 파이프라인 내부에서 미들웨어(middleware) 또는 인터셉터(interceptor) 역할을 합니다.\n",
    "\n",
    "**Filters의 주요 활용 사례(Use Cases):**\n",
    "- **Security / Policy**: 민감한 데이터가 AI 모델로 전송되지 않도록 차단\n",
    "- **Validation**: 함수 실행 전에 파라미터 값 검증\n",
    "- **Error Handling**: 예외를 잡아 기본값을 반환하는 처리\n",
    "- **Logging / Monitoring**: 함수 호출과 응답을 기록 및 모니터링\n",
    "- **Post-processing**: AI에게 반환되기 전에 출력값 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
